{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c49791",
   "metadata": {
    "_cell_guid": "b2941f8c-d092-4f70-a783-28b088a4e0a2",
    "_uuid": "454a80e1-d71e-44af-bcd8-cb0ba22e07a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:50:58.697550Z",
     "iopub.status.busy": "2026-02-10T03:50:58.697316Z",
     "iopub.status.idle": "2026-02-10T03:50:59.486626Z",
     "shell.execute_reply": "2026-02-10T03:50:59.485900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.798093,
     "end_time": "2026-02-10T03:50:59.488859",
     "exception": false,
     "start_time": "2026-02-10T03:50:58.690766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/__results__.html\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/__huggingface_repos__.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/submission.csv\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/__notebook__.ipynb\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/__output__.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/custom.css\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/spm.model\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/trainer_state.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/training_args.bin\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/tokenizer.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/tokenizer_config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/scaler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/scheduler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/model.safetensors\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/special_tokens_map.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/optimizer.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/rng_state.pth\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/added_tokens.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/spm.model\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/trainer_state.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/training_args.bin\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/tokenizer.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/tokenizer_config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/scaler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/scheduler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/model.safetensors\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/special_tokens_map.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/optimizer.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/rng_state.pth\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/added_tokens.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/spm.model\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/training_args.bin\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/tokenizer.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/tokenizer_config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/model.safetensors\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/special_tokens_map.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/added_tokens.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/spm.model\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/trainer_state.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/training_args.bin\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/tokenizer.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/tokenizer_config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/scaler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/scheduler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/model.safetensors\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/special_tokens_map.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/optimizer.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/rng_state.pth\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/added_tokens.json\n",
      "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      "/kaggle/input/llm-classification-finetuning/train.csv\n",
      "/kaggle/input/llm-classification-finetuning/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22de848",
   "metadata": {
    "_cell_guid": "0e57cfa3-96a9-488a-9efc-99158f068043",
    "_uuid": "204f87e6-2eac-4909-8723-31f56713ebed",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:50:59.499191Z",
     "iopub.status.busy": "2026-02-10T03:50:59.498833Z",
     "iopub.status.idle": "2026-02-10T03:50:59.502179Z",
     "shell.execute_reply": "2026-02-10T03:50:59.501652Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00984,
     "end_time": "2026-02-10T03:50:59.503533",
     "exception": false,
     "start_time": "2026-02-10T03:50:59.493693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set this to True when you want to train, False when you just want to submit\n",
    "TRAIN_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7244961d",
   "metadata": {
    "_cell_guid": "497b29de-ecbd-4a53-9abe-ccb3f946e4cc",
    "_uuid": "9c87e9c5-f5b9-44fa-a1a0-151554ed19f9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:50:59.513236Z",
     "iopub.status.busy": "2026-02-10T03:50:59.512715Z",
     "iopub.status.idle": "2026-02-10T03:51:31.488870Z",
     "shell.execute_reply": "2026-02-10T03:51:31.488191Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 31.982774,
     "end_time": "2026-02-10T03:51:31.490551",
     "exception": false,
     "start_time": "2026-02-10T03:50:59.507777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 03:51:15.254407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770695475.443527      23 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770695475.494965      23 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770695475.935805      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770695475.935846      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770695475.935849      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770695475.935851      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e459020",
   "metadata": {
    "_cell_guid": "629fbb6c-5a39-4c48-9f83-86f85b8fb787",
    "_uuid": "f3ddec41-c27a-4d07-b888-84b5a1c1a606",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004304,
     "end_time": "2026-02-10T03:51:31.499362",
     "exception": false,
     "start_time": "2026-02-10T03:51:31.495058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Improvements Made in This Version:\n",
    "1. **Data Augmentation (Swap Trick)** - Double training data by swapping A/B with flipped labels\n",
    "2. **Better XGBoost Features** - Punctuation, URLs, LaTeX/Math, Capitalization ratio\n",
    "3. **Upgraded DeBERTa** - Using deberta-v3-base instead of small\n",
    "4. **Complete Submission Pipeline** - Ready for Kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7601a",
   "metadata": {
    "_cell_guid": "a216f30b-cd76-4c1a-9a7d-4579416f1b40",
    "_uuid": "bb9e4bfb-4096-4665-8e33-2d08140c5667",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004243,
     "end_time": "2026-02-10T03:51:31.507739",
     "exception": false,
     "start_time": "2026-02-10T03:51:31.503496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**JSON & Unicode:** json.loads() is the \"magic bullet\" here. It removes the [\" \"] brackets and automatically converts \\u0411 into the correct Russian/Arabic/Chinese and other language characters in one step.\n",
    "\n",
    "**Code Preservation:** By using isprintable() and only collapsing excessive newlines, we ensure that C, Rust, and Python code blocks keep their indentation and logic.\n",
    "\n",
    "**HTML Safety:** No \"strip HTML\" step here. Because, if the prompt is about HTML, we want to keep those tags. If there are html tags that aren't part of the content, we can add a specific rule for that.\n",
    "\n",
    "**Target Simplification:** Converting the three winner_ columns into a single target (0, 1, 2) makes it much easier to use with DeBERTa or XGBoost later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6001e11c",
   "metadata": {
    "_cell_guid": "b983a151-8427-4d31-acfd-600ae330f6d4",
    "_uuid": "2d26e086-9322-4685-ad8d-34bc57257ea3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:51:31.518000Z",
     "iopub.status.busy": "2026-02-10T03:51:31.517402Z",
     "iopub.status.idle": "2026-02-10T03:51:48.533030Z",
     "shell.execute_reply": "2026-02-10T03:51:48.532056Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 17.022703,
     "end_time": "2026-02-10T03:51:48.534733",
     "exception": false,
     "start_time": "2026-02-10T03:51:31.512030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text columns... this may take a minute.\n",
      "\n",
      "--- Verbosity Bias Analysis ---\n",
      "When Model A wins, it was longer 61.34% of the time.\n",
      "When Model B wins, it was longer 61.59% of the time.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the data\n",
    "df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step A: Handle the [\" \"] wrapping (JSON format)\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, list):\n",
    "            text = \" \".join(data)\n",
    "        else:\n",
    "            text = str(data)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Step B: Remove non-printable control characters (keep newlines and tabs)\n",
    "    text = \"\".join(ch for ch in text if ch.isprintable() or ch in \"\\n\\t\")\n",
    "\n",
    "    # Step C: Standardize Whitespace\n",
    "    text = text.replace(\"\\t\", \"    \")\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 2. Apply cleaning to the main columns\n",
    "print(\"Cleaning text columns... this may take a minute.\")\n",
    "df['prompt'] = df['prompt'].apply(clean_text)\n",
    "df['response_a'] = df['response_a'].apply(clean_text)\n",
    "df['response_b'] = df['response_b'].apply(clean_text)\n",
    "\n",
    "# 3. Create the \"Key Feature\": Length Difference\n",
    "df['len_a'] = df['response_a'].str.len()\n",
    "df['len_b'] = df['response_b'].str.len()\n",
    "df['len_diff'] = df['len_a'] - df['len_b']\n",
    "\n",
    "# 4. Create a single Target column for easier modeling\n",
    "def determine_winner(row):\n",
    "    if row['winner_model_a'] == 1: return 0\n",
    "    if row['winner_model_b'] == 1: return 1\n",
    "    return 2\n",
    "\n",
    "df['target'] = df.apply(determine_winner, axis=1)\n",
    "\n",
    "# 5. Quick EDA on the \"Verbosity Bias\"\n",
    "print(\"\\n--- Verbosity Bias Analysis ---\")\n",
    "a_wins_longer = df[(df['target'] == 0) & (df['len_a'] > df['len_b'])].shape[0]\n",
    "a_wins_total = df[df['target'] == 0].shape[0]\n",
    "print(f\"When Model A wins, it was longer {100 * a_wins_longer / a_wins_total:.2f}% of the time.\")\n",
    "\n",
    "b_wins_longer = df[(df['target'] == 1) & (df['len_b'] > df['len_a'])].shape[0]\n",
    "b_wins_total = df[df['target'] == 1].shape[0]\n",
    "print(f\"When Model B wins, it was longer {100 * b_wins_longer / b_wins_total:.2f}% of the time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1dd3f0",
   "metadata": {
    "_cell_guid": "cce63cf0-63c2-4756-900e-fa5a7896dd81",
    "_uuid": "c41a066c-5f71-4679-b0e2-6f0141ff86ba",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004416,
     "end_time": "2026-02-10T03:51:48.544648",
     "exception": false,
     "start_time": "2026-02-10T03:51:48.540232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The above results are very revealing! A 61%+ win rate for the longer response is a massive signal in machine learning. It confirms that \"Verbosity Bias\" is a dominant factor in this dataset.\n",
    "\n",
    "Here is how to interpret the results and what we should do next to find the \"Positional Bias\" and start modeling.\n",
    "\n",
    "**1. Interpreting your \"Verbosity Bias\"**\n",
    "\n",
    "In a perfectly unbiased world, the longer response should win about 50% of the time. The fact that it wins 61% of the time means:\n",
    "\n",
    "Humans (the judges) are significantly swayed by detail and length.\n",
    "\n",
    "**Your Strategy:** Any model we build must include response length as a feature. Even a simple model that always picks the longer response would likely beat a random guess.\n",
    "\n",
    "**2. How to check for \"Positional Bias\"**\n",
    "Positional bias is the tendency to pick \"Response A\" just because it is the first one the human reads. To check for this, we look at the overall win rates for A vs. B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0b5b7e",
   "metadata": {
    "_cell_guid": "ecec1da3-11da-4406-a155-e3a852eff615",
    "_uuid": "3ed9edf9-f02e-4130-82bc-c391e64a2b6b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:51:48.554971Z",
     "iopub.status.busy": "2026-02-10T03:51:48.554410Z",
     "iopub.status.idle": "2026-02-10T03:51:48.573017Z",
     "shell.execute_reply": "2026-02-10T03:51:48.572229Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025334,
     "end_time": "2026-02-10T03:51:48.574330",
     "exception": false,
     "start_time": "2026-02-10T03:51:48.548996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A Win Rate: 34.91%\n",
      "Model B Win Rate: 34.19%\n",
      "Tie Rate: 30.90%\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall win rates to check positional bias\n",
    "total_rows = len(df)\n",
    "a_wins = df[df['target'] == 0].shape[0]\n",
    "b_wins = df[df['target'] == 1].shape[0]\n",
    "ties = df[df['target'] == 2].shape[0]\n",
    "\n",
    "print(f\"Model A Win Rate: {100 * a_wins / total_rows:.2f}%\")\n",
    "print(f\"Model B Win Rate: {100 * b_wins / total_rows:.2f}%\")\n",
    "print(f\"Tie Rate: {100 * ties / total_rows:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dee25c",
   "metadata": {
    "_cell_guid": "2093241f-5f68-464b-be7c-f49eb2e65bc4",
    "_uuid": "63806163-7c89-4ca4-aa80-8b4adede57e8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004381,
     "end_time": "2026-02-10T03:51:48.583055",
     "exception": false,
     "start_time": "2026-02-10T03:51:48.578674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since the Win Rates are roughly **equal** (e.g., ~34% vs ~34%), then positional bias is low or non-existent in this specific dataset.\n",
    "\n",
    "Now that our data is clean and we understand the biases, it's time to build our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740a7fb",
   "metadata": {
    "_cell_guid": "81d708fc-3efa-48c8-870b-9a9919665e06",
    "_uuid": "e85b5c7b-fe2c-4585-bb33-61ecb6268c9f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004169,
     "end_time": "2026-02-10T03:51:48.591454",
     "exception": false,
     "start_time": "2026-02-10T03:51:48.587285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## STEP 1: Data Augmentation - The \"Swap Trick\"\n",
    "This doubles the training data and forces the model to be position-invariant.\n",
    "\n",
    "**Why it helps:**\n",
    "- Doubles your training size for free\n",
    "- Forces the model to ignore position\n",
    "- Makes predictions more stable and symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2414acf9",
   "metadata": {
    "_cell_guid": "6cec07af-b842-4809-9b68-44f1a570bc0b",
    "_uuid": "135dc766-ae0a-439d-930f-05c281220897",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:51:48.601194Z",
     "iopub.status.busy": "2026-02-10T03:51:48.600662Z",
     "iopub.status.idle": "2026-02-10T03:51:48.655899Z",
     "shell.execute_reply": "2026-02-10T03:51:48.654981Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.061838,
     "end_time": "2026-02-10T03:51:48.657481",
     "exception": false,
     "start_time": "2026-02-10T03:51:48.595643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Data Augmentation (Swap Trick) ---\n",
      "Original data size: 57477\n",
      "Augmented data size: 114954 (2x)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# DATA AUGMENTATION: THE SWAP TRICK\n",
    "# ==========================================\n",
    "print(\"\\n--- Applying Data Augmentation (Swap Trick) ---\")\n",
    "\n",
    "# Create a copy of the dataframe with swapped responses\n",
    "df_swapped = df.copy()\n",
    "df_swapped['response_a'] = df['response_b']\n",
    "df_swapped['response_b'] = df['response_a']\n",
    "\n",
    "# Swap the labels: 0 (A wins) -> 1 (B wins), 1 (B wins) -> 0 (A wins), 2 (Tie) stays the same\n",
    "def swap_label(label):\n",
    "    if label == 0: return 1\n",
    "    if label == 1: return 0\n",
    "    return 2\n",
    "\n",
    "df_swapped['target'] = df_swapped['target'].apply(swap_label)\n",
    "\n",
    "# Swap the length features as well\n",
    "df_swapped['len_a'] = df['len_b']\n",
    "df_swapped['len_b'] = df['len_a']\n",
    "df_swapped['len_diff'] = -df['len_diff']\n",
    "\n",
    "# Combine original and swapped data\n",
    "df_augmented = pd.concat([df, df_swapped], ignore_index=True)\n",
    "print(f\"Original data size: {len(df)}\")\n",
    "print(f\"Augmented data size: {len(df_augmented)} (2x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ed565",
   "metadata": {
    "_cell_guid": "cd4431fa-db7c-4c50-ba7e-032505e309d7",
    "_uuid": "4ae7e2e9-2fa3-4ed4-9d02-ed0912222d3f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004544,
     "end_time": "2026-02-10T03:51:48.666758",
     "exception": false,
     "start_time": "2026-02-10T03:51:48.662214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## STEP 2: Better Features for XGBoost\n",
    "Adding punctuation counts, URL detection, LaTeX/Math detection, capitalization ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c6574a7",
   "metadata": {
    "_cell_guid": "b61544de-ba5a-45fd-b0af-b51c795ac240",
    "_uuid": "6e78135d-55b6-496a-8674-f1d183e33a7c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:51:48.676654Z",
     "iopub.status.busy": "2026-02-10T03:51:48.676377Z",
     "iopub.status.idle": "2026-02-10T03:52:18.221688Z",
     "shell.execute_reply": "2026-02-10T03:52:18.220705Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 29.552288,
     "end_time": "2026-02-10T03:52:18.223383",
     "exception": false,
     "start_time": "2026-02-10T03:51:48.671095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Engineering Enhanced Features ---\n",
      "Total meta features: 36\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ENHANCED FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "print(\"\\n--- Engineering Enhanced Features ---\")\n",
    "\n",
    "def extract_features(df_input):\n",
    "    \"\"\"Extract all features for XGBoost - both original and new 'human-like' features\"\"\"\n",
    "    df_feat = df_input.copy()\n",
    "    \n",
    "    # --- Original Features ---\n",
    "    df_feat['len_a'] = df_feat['response_a'].str.len()\n",
    "    df_feat['len_b'] = df_feat['response_b'].str.len()\n",
    "    df_feat['len_diff'] = df_feat['len_a'] - df_feat['len_b']\n",
    "    \n",
    "    df_feat['word_count_a'] = df_feat['response_a'].apply(lambda x: len(str(x).split()))\n",
    "    df_feat['word_count_b'] = df_feat['response_b'].apply(lambda x: len(str(x).split()))\n",
    "    df_feat['word_diff'] = df_feat['word_count_a'] - df_feat['word_count_b']\n",
    "    \n",
    "    df_feat['unique_words_a'] = df_feat['response_a'].apply(lambda x: len(set(str(x).split())))\n",
    "    df_feat['unique_words_b'] = df_feat['response_b'].apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    df_feat['has_list_a'] = df_feat['response_a'].str.contains(r'\\*|\\d\\.', regex=True).astype(int)\n",
    "    df_feat['has_list_b'] = df_feat['response_b'].str.contains(r'\\*|\\d\\.', regex=True).astype(int)\n",
    "    \n",
    "    # --- NEW: Punctuation Count Features ---\n",
    "    # Count exclamation marks, question marks, colons, semicolons\n",
    "    df_feat['exclaim_a'] = df_feat['response_a'].str.count('!')\n",
    "    df_feat['exclaim_b'] = df_feat['response_b'].str.count('!')\n",
    "    df_feat['exclaim_diff'] = df_feat['exclaim_a'] - df_feat['exclaim_b']\n",
    "    \n",
    "    df_feat['question_a'] = df_feat['response_a'].str.count(r'\\?')\n",
    "    df_feat['question_b'] = df_feat['response_b'].str.count(r'\\?')\n",
    "    \n",
    "    df_feat['colon_a'] = df_feat['response_a'].str.count(':')\n",
    "    df_feat['colon_b'] = df_feat['response_b'].str.count(':')\n",
    "    \n",
    "    # --- NEW: URL/Link Detection ---\n",
    "    # Humans love sources - check for http/https links\n",
    "    url_pattern = r'https?://[^\\s]+'\n",
    "    df_feat['has_url_a'] = df_feat['response_a'].str.contains(url_pattern, regex=True).astype(int)\n",
    "    df_feat['has_url_b'] = df_feat['response_b'].str.contains(url_pattern, regex=True).astype(int)\n",
    "    df_feat['url_count_a'] = df_feat['response_a'].str.count(url_pattern)\n",
    "    df_feat['url_count_b'] = df_feat['response_b'].str.count(url_pattern)\n",
    "    \n",
    "    # --- NEW: LaTeX/Math Detection ---\n",
    "    # Check for $ signs (inline math) and $$ (display math)\n",
    "    df_feat['has_latex_a'] = df_feat['response_a'].str.contains(r'\\$', regex=True).astype(int)\n",
    "    df_feat['has_latex_b'] = df_feat['response_b'].str.contains(r'\\$', regex=True).astype(int)\n",
    "    df_feat['latex_count_a'] = df_feat['response_a'].str.count(r'\\$')\n",
    "    df_feat['latex_count_b'] = df_feat['response_b'].str.count(r'\\$')\n",
    "    \n",
    "    # --- NEW: Capitalization Ratio ---\n",
    "    # High ratio might indicate \"shouting\" or emphasis\n",
    "    def cap_ratio(text):\n",
    "        text = str(text)\n",
    "        if len(text) == 0:\n",
    "            return 0\n",
    "        upper_count = sum(1 for c in text if c.isupper())\n",
    "        return upper_count / len(text)\n",
    "    \n",
    "    df_feat['cap_ratio_a'] = df_feat['response_a'].apply(cap_ratio)\n",
    "    df_feat['cap_ratio_b'] = df_feat['response_b'].apply(cap_ratio)\n",
    "    df_feat['cap_ratio_diff'] = df_feat['cap_ratio_a'] - df_feat['cap_ratio_b']\n",
    "    \n",
    "    # --- NEW: Code Block Detection ---\n",
    "    # Check for code blocks (```)\n",
    "    df_feat['has_code_a'] = df_feat['response_a'].str.contains(r'```', regex=True).astype(int)\n",
    "    df_feat['has_code_b'] = df_feat['response_b'].str.contains(r'```', regex=True).astype(int)\n",
    "    df_feat['code_blocks_a'] = df_feat['response_a'].str.count(r'```')\n",
    "    df_feat['code_blocks_b'] = df_feat['response_b'].str.count(r'```')\n",
    "    \n",
    "    # --- NEW: Sentence Count (Proxy for Structure) ---\n",
    "    df_feat['sentence_count_a'] = df_feat['response_a'].str.count(r'[.!?]+')\n",
    "    df_feat['sentence_count_b'] = df_feat['response_b'].str.count(r'[.!?]+')\n",
    "    \n",
    "    # --- NEW: Newline Count (Formatting/Readability) ---\n",
    "    df_feat['newline_a'] = df_feat['response_a'].str.count('\\n')\n",
    "    df_feat['newline_b'] = df_feat['response_b'].str.count('\\n')\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "# Apply feature extraction to augmented data\n",
    "df_augmented = extract_features(df_augmented)\n",
    "\n",
    "# Define the meta features list (expanded)\n",
    "meta_features = [\n",
    "    'len_a', 'len_b', 'len_diff', \n",
    "    'word_count_a', 'word_count_b', 'word_diff',\n",
    "    'unique_words_a', 'unique_words_b',\n",
    "    'has_list_a', 'has_list_b',\n",
    "    # New features\n",
    "    'exclaim_a', 'exclaim_b', 'exclaim_diff',\n",
    "    'question_a', 'question_b',\n",
    "    'colon_a', 'colon_b',\n",
    "    'has_url_a', 'has_url_b', 'url_count_a', 'url_count_b',\n",
    "    'has_latex_a', 'has_latex_b', 'latex_count_a', 'latex_count_b',\n",
    "    'cap_ratio_a', 'cap_ratio_b', 'cap_ratio_diff',\n",
    "    'has_code_a', 'has_code_b', 'code_blocks_a', 'code_blocks_b',\n",
    "    'sentence_count_a', 'sentence_count_b',\n",
    "    'newline_a', 'newline_b'\n",
    "]\n",
    "\n",
    "print(f\"Total meta features: {len(meta_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9c178",
   "metadata": {
    "_cell_guid": "748510c7-70c2-41b5-915d-757e4080e9c6",
    "_uuid": "8286d7bf-3e1f-4cdc-b932-9c808c48e385",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00475,
     "end_time": "2026-02-10T03:52:18.233316",
     "exception": false,
     "start_time": "2026-02-10T03:52:18.228566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What is TF-IDF doing here?**\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) identifies which words are unique or important to a specific response. For example:\n",
    "\n",
    "If Response A uses words like \"accurate,\" \"detailed,\" and \"verified,\" while Response B uses \"sorry,\" \"cannot,\" and \"error,\" TF-IDF will give those words high scores.\n",
    "The model will then learn that \"sorry\" is a strong signal for losing, while \"detailed\" is a signal for winning.\n",
    "\n",
    "**The Strategy: The \"Vector Difference\" Trick**\n",
    "\n",
    "Since we are comparing two responses, we don't just want to know what words are in A. We want to know what words are in A that are NOT in B.\n",
    "\n",
    "We turn Response A into a vector of numbers.\n",
    "We turn Response B into a vector of numbers.\n",
    "We subtract them: X = Vector_A - Vector_B.\n",
    "This \"Difference Vector\" tells the model exactly what the content gap is between the two bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6b4470f",
   "metadata": {
    "_cell_guid": "5758f643-ebfd-4636-bc24-a41929ad1c17",
    "_uuid": "1bf4267c-438f-4c44-8f2f-10084d93c4d7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:52:18.243779Z",
     "iopub.status.busy": "2026-02-10T03:52:18.243351Z",
     "iopub.status.idle": "2026-02-10T03:53:55.906378Z",
     "shell.execute_reply": "2026-02-10T03:53:55.905692Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 97.674753,
     "end_time": "2026-02-10T03:53:55.912691",
     "exception": false,
     "start_time": "2026-02-10T03:52:18.237938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building TF-IDF Features ---\n",
      "Final feature matrix shape: (114954, 5036)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# TF-IDF VECTORIZATION\n",
    "# ================================================================\n",
    "print(\"\\n--- Building TF-IDF Features ---\")\n",
    "\n",
    "# Combine all text for fitting TF-IDF (use original df to avoid data leakage)\n",
    "all_text = pd.concat([df['prompt'], df['response_a'], df['response_b']])\n",
    "\n",
    "# Initialize TF-IDF with improved parameters\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    ngram_range=(1, 2), \n",
    "    stop_words='english',\n",
    "    min_df=3,  # Ignore rare terms\n",
    "    max_df=0.95  # Ignore too common terms\n",
    ")\n",
    "tfidf.fit(all_text)\n",
    "\n",
    "# Transform the augmented data\n",
    "X_a = tfidf.transform(df_augmented['response_a'])\n",
    "X_b = tfidf.transform(df_augmented['response_b'])\n",
    "X_diff = X_a - X_b\n",
    "\n",
    "# Combine TF-IDF with meta features\n",
    "X_final = hstack([X_diff, csr_matrix(df_augmented[meta_features].values)])\n",
    "\n",
    "print(f\"Final feature matrix shape: {X_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7a6057",
   "metadata": {
    "_cell_guid": "cf6183e6-ce98-46c4-84ef-53359c1d28e4",
    "_uuid": "f5adfd53-eefb-4cfe-8375-b208f887193c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:53:55.923083Z",
     "iopub.status.busy": "2026-02-10T03:53:55.922831Z",
     "iopub.status.idle": "2026-02-10T03:53:56.030101Z",
     "shell.execute_reply": "2026-02-10T03:53:56.028958Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.114674,
     "end_time": "2026-02-10T03:53:56.031878",
     "exception": false,
     "start_time": "2026-02-10T03:53:55.917204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating Train/Val Split ---\n",
      "Training samples: 91964 (augmented)\n",
      "Validation samples: 11495 (original only)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# TRAIN/VAL SPLIT (Using Original Data for Validation)\n",
    "# ==========================================\n",
    "print(\"\\n--- Creating Train/Val Split ---\")\n",
    "\n",
    "# Important: We use ONLY original data indices for validation\n",
    "# to get a fair score estimate (no augmented data in validation)\n",
    "original_size = len(df)\n",
    "augmented_size = len(df_augmented)\n",
    "\n",
    "# Add orig_id for alignment\n",
    "df_augmented['orig_id'] = np.arange(len(df_augmented))\n",
    "\n",
    "# Rename target to labels for Trainer compatibility\n",
    "df_augmented = df_augmented.rename(columns={'target': 'labels'})\n",
    "\n",
    "# Split: Use original data for validation (indices 0 to original_size-1)\n",
    "# We'll use 20% of original data for validation\n",
    "np.random.seed(42)\n",
    "original_indices = np.arange(original_size)\n",
    "np.random.shuffle(original_indices)\n",
    "\n",
    "val_size = int(0.2 * original_size)\n",
    "val_indices = original_indices[:val_size]\n",
    "train_indices_original = original_indices[val_size:]\n",
    "\n",
    "# For training, use both original training indices AND their augmented counterparts\n",
    "# Augmented versions of original data are at indices original_size + original_index\n",
    "train_indices_augmented = train_indices_original + original_size\n",
    "train_indices = np.concatenate([train_indices_original, train_indices_augmented])\n",
    "\n",
    "# Create splits\n",
    "X_train = X_final.tocsr()[train_indices]\n",
    "X_val = X_final.tocsr()[val_indices]\n",
    "\n",
    "y_train = df_augmented.loc[train_indices, 'labels'].values\n",
    "y_val = df_augmented.loc[val_indices, 'labels'].values\n",
    "\n",
    "print(f\"Training samples: {len(train_indices)} (augmented)\")\n",
    "print(f\"Validation samples: {len(val_indices)} (original only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef6b7a",
   "metadata": {
    "_cell_guid": "7e7537df-802e-44ae-9ae0-3d35960bf6a0",
    "_uuid": "ee042ee0-1279-4332-8ee5-293064a44c8d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005009,
     "end_time": "2026-02-10T03:53:56.041896",
     "exception": false,
     "start_time": "2026-02-10T03:53:56.036887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Why XGBoost instead of Logistic Regression?**\n",
    "\n",
    "XGBoost is much better than Logistic Regression for this because:\n",
    "\n",
    "**Feature Selection:** XGBoost automatically ignores features that don't help. It will likely ignore many of those TF-IDF words and focus on the ones that actually matter.\n",
    "\n",
    "**Non-Linearity:** It can understand that \"Length matters, but only if the response also contains certain keywords.\"\n",
    "\n",
    "We've also added \"Human-Centric Features\" (punctuation, URLs, LaTeX, capitalization) to help the model understand quality signals that humans look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebce074b",
   "metadata": {
    "_cell_guid": "145d9a95-97d3-45cd-acd7-c93671f59d37",
    "_uuid": "f5dbb560-f983-4151-a159-a74fef7824dd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:53:56.052439Z",
     "iopub.status.busy": "2026-02-10T03:53:56.052178Z",
     "iopub.status.idle": "2026-02-10T03:55:49.472110Z",
     "shell.execute_reply": "2026-02-10T03:55:49.471333Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 113.427029,
     "end_time": "2026-02-10T03:55:49.473537",
     "exception": false,
     "start_time": "2026-02-10T03:53:56.046508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training XGBoost ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [03:55:49] WARNING: /workspace/src/common/error_msg.cc:41: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Log Loss: 1.0166\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# TRAIN XGBOOST\n",
    "# ================================================================\n",
    "print(\"\\n--- Training XGBoost ---\")\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    objective='multi:softprob',\n",
    "    num_class=3,\n",
    "    tree_method='hist', \n",
    "    device='cuda',\n",
    "    random_state=42,\n",
    "    # Additional regularization\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8\n",
    ")\n",
    "\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "probs_xgb = model_xgb.predict_proba(X_val)\n",
    "xgb_score = log_loss(y_val, probs_xgb)\n",
    "print(f\"XGBoost Log Loss: {xgb_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2f9b4",
   "metadata": {
    "_cell_guid": "59647274-406a-408e-90ca-bd04ec75d848",
    "_uuid": "104137b1-839d-41b1-a81b-a6eb234d63d1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004985,
     "end_time": "2026-02-10T03:55:49.483392",
     "exception": false,
     "start_time": "2026-02-10T03:55:49.478407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## STEP 3: Upgrade DeBERTa to deberta-v3-base\n",
    "\n",
    "**The Next Level: Tier 3 - DeBERTa-v3 (The Gold Standard)**\n",
    "\n",
    "To get the score below 1.00 (and eventually toward 0.90 or lower), we need a model that actually understands language. This is where DeBERTa-v3 comes in.\n",
    "\n",
    "**What makes DeBERTa-v3 different?**\n",
    "Unlike XGBoost, which looks at a table of numbers, DeBERTa \"reads\" the text using Self-Attention. It looks at every word in the context of every other word.\n",
    "\n",
    "**The \"Cross-Encoder\" Strategy**\n",
    "In this competition, the most powerful way to use DeBERTa is as a Cross-Encoder. Instead of looking at Response A and Response B separately, we feed them to the model together so it can compare them side-by-side.\n",
    "\n",
    "**We format the input like this:**\n",
    "[CLS] Prompt [SEP] Response A [SEP] Response B [SEP]\n",
    "\n",
    "The model then uses its attention mechanism to \"look back and forth\" between A and B to decide which one answers the Prompt better.\n",
    "\n",
    "**Upgrade from small to base:**\n",
    "- `deberta-v3-small` is the \"Lite\" version\n",
    "- `deberta-v3-base` is much larger and more powerful\n",
    "- Cost: Reduce `per_device_train_batch_size` to 2 and increase `gradient_accumulation_steps` to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b09d431",
   "metadata": {
    "_cell_guid": "89158d0b-748a-4fb1-8fc5-0e024621b983",
    "_uuid": "ab6dbaaf-930c-4532-987c-83a90b6e8b7b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:55:49.495656Z",
     "iopub.status.busy": "2026-02-10T03:55:49.495049Z",
     "iopub.status.idle": "2026-02-10T03:55:49.498778Z",
     "shell.execute_reply": "2026-02-10T03:55:49.498089Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010963,
     "end_time": "2026-02-10T03:55:49.500229",
     "exception": false,
     "start_time": "2026-02-10T03:55:49.489266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# DEBERTA CONFIGURATION\n",
    "# ==========================================\n",
    "# UPGRADED: Using deberta-v3-base instead of small\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"  # UPGRADED from deberta-v3-small\n",
    "# CHECKPOINT_PATH = \"/kaggle/input/llm-classifier-finetuning-continued/results/checkpoint-5748/\"\n",
    "\n",
    "# For inference mode, you may need to update this path to your new base model checkpoint\n",
    "CHECKPOINT_PATH = \"/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000\"  # Update after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e15632",
   "metadata": {
    "_cell_guid": "d29f93fe-99d4-4266-bb9a-747ff5c53025",
    "_uuid": "c1716bdc-a3c1-4067-aabe-0fe09092e803",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:55:49.510855Z",
     "iopub.status.busy": "2026-02-10T03:55:49.510567Z",
     "iopub.status.idle": "2026-02-10T03:57:43.629721Z",
     "shell.execute_reply": "2026-02-10T03:57:43.628798Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 114.126156,
     "end_time": "2026-02-10T03:57:43.631241",
     "exception": false,
     "start_time": "2026-02-10T03:55:49.505085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing DeBERTa Dataset ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee2848ada0845e2a9ad993f9397a208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16345fb45aa4eefaa1f9324ed505bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train size: 91964\n",
      "Tokenized val size: 11495\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PREPARE DEBERTA DATASET (WITH AUGMENTATION)\n",
    "# ==========================================\n",
    "print(\"\\n--- Preparing DeBERTa Dataset ---\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"{p} [SEP] {a} [SEP] {b}\" \n",
    "        for p, a, b in zip(examples['prompt'], examples['response_a'], examples['response_b'])\n",
    "    ]\n",
    "    # OPTIMIZED: Reduced from 1024 to 512 - cuts training time significantly\n",
    "    # 512 tokens captures most response content while being 4x faster\n",
    "    return tokenizer(inputs, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# Create datasets for train and validation\n",
    "# Training uses augmented data\n",
    "train_df = df_augmented.iloc[train_indices][['prompt', 'response_a', 'response_b', 'labels', 'orig_id']]\n",
    "val_df = df_augmented.iloc[val_indices][['prompt', 'response_a', 'response_b', 'labels', 'orig_id']]\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "# Tokenize\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(f\"Tokenized train size: {len(tokenized_train)}\")\n",
    "print(f\"Tokenized val size: {len(tokenized_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377168bd",
   "metadata": {
    "_cell_guid": "9a5313f1-22cf-42b0-88a0-fe8b35b3fae8",
    "_uuid": "5317ebc5-40da-4dc6-8794-8126e7b4aee5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:57:43.642950Z",
     "iopub.status.busy": "2026-02-10T03:57:43.642726Z",
     "iopub.status.idle": "2026-02-10T03:57:52.320619Z",
     "shell.execute_reply": "2026-02-10T03:57:52.319958Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8.685738,
     "end_time": "2026-02-10T03:57:52.322353",
     "exception": false,
     "start_time": "2026-02-10T03:57:43.636615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MODE: INFERENCE (Loading Checkpoint) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1800854325.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# TRAIN/LOAD DEBERTA\n",
    "# ==========================================\n",
    "if TRAIN_MODEL:\n",
    "    print(\"\\n--- MODE: RESUMING TRAINING DeBERTa-v3-base ---\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_PATH, num_labels=3)\n",
    "    \n",
    "    # OPTIMIZED FOR: 2x T4 GPUs + doubled data + deberta-v3-base\n",
    "    # Estimated training time: ~8-10 hours with max_steps limit\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,                    # Slightly higher for faster convergence\n",
    "        per_device_train_batch_size=4,         # 4 per GPU x 2 GPUs = 8 total\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=2,                    # Reduced from 3.5 - usually sufficient\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"steps\",                 # More frequent evaluation\n",
    "        eval_steps=500,                        # Evaluate every 500 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,                        # Checkpoint every 500 steps\n",
    "        save_total_limit=3,                    # Keep last 3 checkpoints\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=4,         # Effective batch = 4*4*2 = 32\n",
    "        report_to=\"none\",\n",
    "        warmup_ratio=0.06,                     # Less warmup for fewer epochs\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        max_steps=6000,                        # SAFETY LIMIT - prevents timeout\n",
    "        logging_steps=100,                     # Monitor progress\n",
    "        dataloader_num_workers=4,              # Faster data loading\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n--- MODE: INFERENCE (Loading Checkpoint) ---\")\n",
    "    # Note: Update CHECKPOINT_PATH to your trained base model checkpoint\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_PATH)\n",
    "    except:\n",
    "        print(\"Warning: Checkpoint not found, loading from base model\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./temp\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        fp16=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8ee39a7",
   "metadata": {
    "_cell_guid": "645c0da2-cacd-4b95-98db-85e14346a12a",
    "_uuid": "203f02b4-c666-40fa-9c29-dea4a9b755b7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T03:57:52.334365Z",
     "iopub.status.busy": "2026-02-10T03:57:52.334121Z",
     "iopub.status.idle": "2026-02-10T04:04:12.471828Z",
     "shell.execute_reply": "2026-02-10T04:04:12.471216Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 380.145362,
     "end_time": "2026-02-10T04:04:12.473303",
     "exception": false,
     "start_time": "2026-02-10T03:57:52.327941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping training.\n",
      "\n",
      "Generating DeBERTa predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# EXECUTE TRAINING\n",
    "# ==========================================\n",
    "if TRAIN_MODEL:\n",
    "    print(\"Fresh training since the learning rate is changed but the previous model weights from the checkpoint-5748 will still be used, but the optimizer/scheduler will reset  which is what we want with new hyperparameters\")\n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.save_model(\"./results/final_model\")\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"DeBERTa Validation Log Loss: {metrics['eval_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping training.\")\n",
    "\n",
    "# Get DeBERTa predictions on validation set\n",
    "print(\"\\nGenerating DeBERTa predictions...\")\n",
    "deberta_output = trainer.predict(tokenized_val)\n",
    "deberta_probs = softmax(deberta_output.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "082e17da",
   "metadata": {
    "_cell_guid": "aa23936d-1ed0-463c-8f96-be093b3c9565",
    "_uuid": "c7a64990-1c0b-4f4e-9b3f-ccc3cc5499da",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T04:04:12.485778Z",
     "iopub.status.busy": "2026-02-10T04:04:12.485303Z",
     "iopub.status.idle": "2026-02-10T04:04:12.545220Z",
     "shell.execute_reply": "2026-02-10T04:04:12.544241Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.069221,
     "end_time": "2026-02-10T04:04:12.548194",
     "exception": false,
     "start_time": "2026-02-10T04:04:12.478973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Searching for Optimal Blend ---\n",
      "XGB Weight 0.00: Log Loss 1.4589\n",
      "XGB Weight 0.50: Log Loss 1.0453\n",
      "XGB Weight 1.00: Log Loss 1.0166\n",
      "\n",
      " Best Weight: 0.85 XGB / 0.15 DeBERTa\n",
      " Final Blended Log Loss: 1.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# FIND OPTIMAL BLEND\n",
    "# ==========================================\n",
    "print(\"\\n--- Searching for Optimal Blend ---\")\n",
    "best_score = 999\n",
    "best_w = 0\n",
    "\n",
    "for w in np.linspace(0, 1, 21):  # More granular search: 0.0, 0.05, 0.10 ... 1.0\n",
    "    blend = (w * probs_xgb) + ((1-w) * deberta_probs)\n",
    "    score = log_loss(y_val, blend)\n",
    "    if w in [0.0, 0.3, 0.5, 0.7, 1.0]:  # Print selected weights\n",
    "        print(f\"XGB Weight {w:.2f}: Log Loss {score:.4f}\")\n",
    "    \n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_w = w\n",
    "\n",
    "print(f\"\\n Best Weight: {best_w:.2f} XGB / {1-best_w:.2f} DeBERTa\")\n",
    "print(f\" Final Blended Log Loss: {best_score:.4f}\")\n",
    "\n",
    "# Store the best weight for submission\n",
    "BEST_XGB_WEIGHT = best_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192fbbb",
   "metadata": {
    "_cell_guid": "4cf7477c-73d3-4e58-8f4a-9ef1b56a11cc",
    "_uuid": "c653bc67-9f34-4593-8f99-604cd7ae55aa",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00565,
     "end_time": "2026-02-10T04:04:12.559689",
     "exception": false,
     "start_time": "2026-02-10T04:04:12.554039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## STEP 4: Create Kaggle Submission\n",
    "\n",
    "**The Final Boss  Making a Submission**\n",
    "\n",
    "Before you improve the score, you should learn how to actually submit to Kaggle. The test.csv file has no labels. You must:\n",
    "\n",
    "1. Load test.csv\n",
    "2. Run the same cleaning and feature engineering\n",
    "3. Get probabilities from XGBoost and DeBERTa\n",
    "4. Blend them using your optimal weights\n",
    "5. Save a submission.csv with columns: `id`, `winner_model_a`, `winner_model_b`, `winner_tie`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83e57c1",
   "metadata": {
    "_cell_guid": "b0d541c8-a9f2-4628-9699-8f9f8def6697",
    "_uuid": "14062805-fad4-4df5-ae33-31d50653ac41",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T04:04:12.572265Z",
     "iopub.status.busy": "2026-02-10T04:04:12.571483Z",
     "iopub.status.idle": "2026-02-10T04:04:12.908593Z",
     "shell.execute_reply": "2026-02-10T04:04:12.907739Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.345035,
     "end_time": "2026-02-10T04:04:12.910079",
     "exception": false,
     "start_time": "2026-02-10T04:04:12.565044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATING SUBMISSION\n",
      "==================================================\n",
      "Test data size: 3\n",
      "Cleaning test data...\n",
      "Extracting features...\n",
      "Getting XGBoost predictions...\n",
      "Getting DeBERTa predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116d383f5fdb40f2943b0176a71703af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending with weights: 0.85 XGB / 0.15 DeBERTa\n",
      "\n",
      " Submission file saved: submission.csv\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.272837        0.388777    0.338385\n",
      "1   211333        0.376775        0.258851    0.364374\n",
      "2  1233961        0.311213        0.310040    0.378747\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SUBMISSION PIPELINE\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING SUBMISSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Load the Test Data\n",
    "test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n",
    "print(f\"Test data size: {len(test_df)}\")\n",
    "\n",
    "# 2. Apply the SAME cleaning function\n",
    "print(\"Cleaning test data...\")\n",
    "test_df['prompt'] = test_df['prompt'].apply(clean_text)\n",
    "test_df['response_a'] = test_df['response_a'].apply(clean_text)\n",
    "test_df['response_b'] = test_df['response_b'].apply(clean_text)\n",
    "\n",
    "# 3. Generate ALL features for test set (using the same function)\n",
    "print(\"Extracting features...\")\n",
    "test_df = extract_features(test_df)\n",
    "\n",
    "# 4. Transform Text using the ALREADY FITTED TF-IDF\n",
    "X_test_a = tfidf.transform(test_df['response_a'])\n",
    "X_test_b = tfidf.transform(test_df['response_b'])\n",
    "X_test_diff = X_test_a - X_test_b\n",
    "\n",
    "# Combine with meta features\n",
    "X_test_final = hstack([X_test_diff, csr_matrix(test_df[meta_features].values)])\n",
    "\n",
    "# 5. Get XGBoost Predictions\n",
    "print(\"Getting XGBoost predictions...\")\n",
    "xgb_test_probs = model_xgb.predict_proba(X_test_final)\n",
    "\n",
    "# 6. Get DeBERTa Predictions\n",
    "print(\"Getting DeBERTa predictions...\")\n",
    "test_ds = Dataset.from_pandas(test_df[['prompt', 'response_a', 'response_b']])\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "deberta_test_output = trainer.predict(tokenized_test)\n",
    "deberta_test_probs = softmax(deberta_test_output.predictions, axis=1)\n",
    "\n",
    "# 7. Apply the Optimal Blend\n",
    "print(f\"Blending with weights: {BEST_XGB_WEIGHT:.2f} XGB / {1-BEST_XGB_WEIGHT:.2f} DeBERTa\")\n",
    "final_probs = (BEST_XGB_WEIGHT * xgb_test_probs) + ((1-BEST_XGB_WEIGHT) * deberta_test_probs)\n",
    "\n",
    "# 8. Create Submission File\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'winner_model_a': final_probs[:, 0],\n",
    "    'winner_model_b': final_probs[:, 1],\n",
    "    'winner_tie': final_probs[:, 2]\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n Submission file saved: submission.csv\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138547b",
   "metadata": {
    "_cell_guid": "defc8e55-e897-4aa1-a0df-8bb3407ecd29",
    "_uuid": "b279e6a6-ec8d-4cf8-ad7e-c8727f41d09c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00604,
     "end_time": "2026-02-10T04:04:12.922400",
     "exception": false,
     "start_time": "2026-02-10T04:04:12.916360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary of Changes Made:\n",
    "\n",
    "### 1. Data Augmentation (Swap Trick)\n",
    "- Created `df_swapped` by swapping response_a and response_b\n",
    "- Flipped labels accordingly (A wins  B wins, Tie stays same)\n",
    "- Combined original + swapped = 2x training data\n",
    "\n",
    "### 2. Better XGBoost Features\n",
    "- **Punctuation counts**: exclamation marks, question marks, colons\n",
    "- **URL detection**: `has_url_a/b`, `url_count_a/b`\n",
    "- **LaTeX/Math detection**: `has_latex_a/b`, `latex_count_a/b`\n",
    "- **Capitalization ratio**: `cap_ratio_a/b/diff`\n",
    "- **Code block detection**: `has_code_a/b`, `code_blocks_a/b`\n",
    "- **Structure features**: `sentence_count_a/b`, `newline_a/b`\n",
    "\n",
    "### 3. Upgraded DeBERTa\n",
    "- Changed from `deberta-v3-small` to `deberta-v3-base`\n",
    "- Reduced `per_device_train_batch_size` from 4 to 2\n",
    "- Increased `gradient_accumulation_steps` from 4 to 8\n",
    "- Added warmup and cosine learning rate schedule\n",
    "\n",
    "### 4. Submission Pipeline\n",
    "- Complete pipeline for processing test.csv\n",
    "- Uses optimal blend weights found during validation\n",
    "- Outputs properly formatted submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7747e34a",
   "metadata": {
    "_cell_guid": "ce891917-37f3-47c0-81d5-0f674e149bfe",
    "_uuid": "c088f435-a674-4e02-9578-39f1dce1931b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T04:04:12.935546Z",
     "iopub.status.busy": "2026-02-10T04:04:12.934983Z",
     "iopub.status.idle": "2026-02-10T04:04:12.940233Z",
     "shell.execute_reply": "2026-02-10T04:04:12.939173Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013278,
     "end_time": "2026-02-10T04:04:12.941573",
     "exception": false,
     "start_time": "2026-02-10T04:04:12.928295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL SUMMARY\n",
      "==================================================\n",
      "Training data size: 91964 (with augmentation)\n",
      "Validation data size: 11495\n",
      "Total features: 5036\n",
      "XGBoost Log Loss: 1.0166\n",
      "Optimal blend: 0.85 XGB / 0.15 DeBERTa\n",
      "Best blended Log Loss: 1.0092\n"
     ]
    }
   ],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training data size: {len(train_indices)} (with augmentation)\")\n",
    "print(f\"Validation data size: {len(val_indices)}\")\n",
    "print(f\"Total features: {X_final.shape[1]}\")\n",
    "print(f\"XGBoost Log Loss: {xgb_score:.4f}\")\n",
    "print(f\"Optimal blend: {BEST_XGB_WEIGHT:.2f} XGB / {1-BEST_XGB_WEIGHT:.2f} DeBERTa\")\n",
    "print(f\"Best blended Log Loss: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41b4447d",
   "metadata": {
    "_cell_guid": "deabb952-ec2f-45ba-857b-dc913d3c2fee",
    "_uuid": "90e52f8d-0563-4b0b-8d31-9d03abfe07a8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-10T04:04:12.954849Z",
     "iopub.status.busy": "2026-02-10T04:04:12.954436Z",
     "iopub.status.idle": "2026-02-10T04:04:12.968311Z",
     "shell.execute_reply": "2026-02-10T04:04:12.966942Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021928,
     "end_time": "2026-02-10T04:04:12.969743",
     "exception": false,
     "start_time": "2026-02-10T04:04:12.947815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/__results__.html\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/__huggingface_repos__.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/submission.csv\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/__notebook__.ipynb\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/__output__.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/custom.css\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/spm.model\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/trainer_state.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/training_args.bin\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/tokenizer.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/tokenizer_config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/scaler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/scheduler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/model.safetensors\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/special_tokens_map.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/optimizer.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/rng_state.pth\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-5500/added_tokens.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/spm.model\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/trainer_state.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/training_args.bin\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/tokenizer.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/tokenizer_config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/scaler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/scheduler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/model.safetensors\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/special_tokens_map.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/optimizer.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/rng_state.pth\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-500/added_tokens.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/spm.model\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/training_args.bin\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/tokenizer.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/tokenizer_config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/model.safetensors\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/special_tokens_map.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/final_model/added_tokens.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/spm.model\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/trainer_state.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/training_args.bin\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/tokenizer.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/tokenizer_config.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/scaler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/scheduler.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/model.safetensors\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/special_tokens_map.json\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/optimizer.pt\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/rng_state.pth\n",
      "/kaggle/input/notebooks/prabhaharan/llm-classifier-finetuning-continued-2/results/checkpoint-6000/added_tokens.json\n",
      "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      "/kaggle/input/llm-classification-finetuning/train.csv\n",
      "/kaggle/input/llm-classification-finetuning/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b236b55",
   "metadata": {
    "_cell_guid": "85db1169-5ae2-4c44-ae88-0d4f8eb8677a",
    "_uuid": "12cf6ff1-b455-428b-b8d4-f693d81aee35",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00585,
     "end_time": "2026-02-10T04:04:12.981429",
     "exception": false,
     "start_time": "2026-02-10T04:04:12.975579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "isSourceIdPinned": false,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "sourceId": 296155149,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 800.329847,
   "end_time": "2026-02-10T04:04:16.444954",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-10T03:50:56.115107",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a6a91da6053423f856f6aab9b31e14b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "10264321da2a4b2dbced4f07bcb159cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_87e014c1f12b4cacb9a260bb7542cea3",
       "max": 11495.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_309d5225b40c4795bc5fbc1abad472f0",
       "tabbable": null,
       "tooltip": null,
       "value": 11495.0
      }
     },
     "116d383f5fdb40f2943b0176a71703af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a25bbbce6d53490da936d6d0d732ec64",
        "IPY_MODEL_4f211246609d430793f22acb4754a0cb",
        "IPY_MODEL_6ef71a8aa43b47ccba274de8b6d39cb6"
       ],
       "layout": "IPY_MODEL_5006208623bd41bdbb7b9f46f86c5c9c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "298db773c1f24e7b989d416f69561a55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "309d5225b40c4795bc5fbc1abad472f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3fa1f97a65e6405bb80e668979c85239": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9f2c9c184f2b41118a797c244daeb4f3",
       "placeholder": "",
       "style": "IPY_MODEL_fc256051599746659d9d2664b69d2c0a",
       "tabbable": null,
       "tooltip": null,
       "value": "91964/91964[01:39&lt;00:00,904.03examples/s]"
      }
     },
     "4f211246609d430793f22acb4754a0cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cd06557c7f1a4856b8d62060f13494ae",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_298db773c1f24e7b989d416f69561a55",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "5006208623bd41bdbb7b9f46f86c5c9c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d02ea1b1247442a860e0b2a0d256058": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6503e3b9cd3448a3a8148e0a0d4f3355": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6e3f88008b6544ebbea59e62d7f269fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f16d83df9ade463185f5b0b34dd7ff85",
       "placeholder": "",
       "style": "IPY_MODEL_6503e3b9cd3448a3a8148e0a0d4f3355",
       "tabbable": null,
       "tooltip": null,
       "value": "Map:100%"
      }
     },
     "6ef71a8aa43b47ccba274de8b6d39cb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0a6a91da6053423f856f6aab9b31e14b",
       "placeholder": "",
       "style": "IPY_MODEL_8e1f1cd4bfcc443d8ac816601b3dc45a",
       "tabbable": null,
       "tooltip": null,
       "value": "3/3[00:00&lt;00:00,127.57examples/s]"
      }
     },
     "79b18b9f5358458883ff6fd1a24d3e0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7c1bedc577f04668ac77ec6d2291b374": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_af920767a2504c4ba89945cb6d2083fc",
       "placeholder": "",
       "style": "IPY_MODEL_fc750efdbe644863877ecaef5a47db4b",
       "tabbable": null,
       "tooltip": null,
       "value": "Map:100%"
      }
     },
     "7da96668737440039c6e3eb94b7b260d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80318ee8ace4402bac3f0f2f316be1eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "87e014c1f12b4cacb9a260bb7542cea3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e1f1cd4bfcc443d8ac816601b3dc45a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d1fd98570af427db6ee088d1ae4f7d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f2c9c184f2b41118a797c244daeb4f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a25bbbce6d53490da936d6d0d732ec64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_79b18b9f5358458883ff6fd1a24d3e0e",
       "placeholder": "",
       "style": "IPY_MODEL_80318ee8ace4402bac3f0f2f316be1eb",
       "tabbable": null,
       "tooltip": null,
       "value": "Map:100%"
      }
     },
     "aee2848ada0845e2a9ad993f9397a208": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6e3f88008b6544ebbea59e62d7f269fc",
        "IPY_MODEL_b635b18bfbf24249901112aa1638603c",
        "IPY_MODEL_3fa1f97a65e6405bb80e668979c85239"
       ],
       "layout": "IPY_MODEL_9d1fd98570af427db6ee088d1ae4f7d0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "af920767a2504c4ba89945cb6d2083fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b635b18bfbf24249901112aa1638603c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7da96668737440039c6e3eb94b7b260d",
       "max": 91964.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5d02ea1b1247442a860e0b2a0d256058",
       "tabbable": null,
       "tooltip": null,
       "value": 91964.0
      }
     },
     "bf613a3a3bd84da8af1e877b4ec977c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cd06557c7f1a4856b8d62060f13494ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df21e5a3b076488d95d399b6e004d858": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e16345fb45aa4eefaa1f9324ed505bbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7c1bedc577f04668ac77ec6d2291b374",
        "IPY_MODEL_10264321da2a4b2dbced4f07bcb159cc",
        "IPY_MODEL_e2620625c8ae455fa1ef7ab599b890bf"
       ],
       "layout": "IPY_MODEL_df21e5a3b076488d95d399b6e004d858",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e2620625c8ae455fa1ef7ab599b890bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f23d5d22eda44a9ba5666c7c7c90e1d3",
       "placeholder": "",
       "style": "IPY_MODEL_bf613a3a3bd84da8af1e877b4ec977c2",
       "tabbable": null,
       "tooltip": null,
       "value": "11495/11495[00:12&lt;00:00,931.47examples/s]"
      }
     },
     "f16d83df9ade463185f5b0b34dd7ff85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f23d5d22eda44a9ba5666c7c7c90e1d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc256051599746659d9d2664b69d2c0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fc750efdbe644863877ecaef5a47db4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
